{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pydot as pt\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', low_memory = False)\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df = train_df.drop('ID_code', axis = 1)\n",
    "test_df = test_df.drop('ID_code', axis = 1)\n",
    "\n",
    "train_df = train_df.apply(pd.to_numeric)\n",
    "test_df = test_df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.100490</td>\n",
       "      <td>10.679914</td>\n",
       "      <td>-1.627622</td>\n",
       "      <td>10.715192</td>\n",
       "      <td>6.796529</td>\n",
       "      <td>11.078333</td>\n",
       "      <td>-5.065317</td>\n",
       "      <td>5.408949</td>\n",
       "      <td>16.545850</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>...</td>\n",
       "      <td>3.234440</td>\n",
       "      <td>7.438408</td>\n",
       "      <td>1.927839</td>\n",
       "      <td>3.331774</td>\n",
       "      <td>17.993784</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>2.303335</td>\n",
       "      <td>8.908158</td>\n",
       "      <td>15.870720</td>\n",
       "      <td>-3.326537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.300653</td>\n",
       "      <td>3.040051</td>\n",
       "      <td>4.050044</td>\n",
       "      <td>2.640894</td>\n",
       "      <td>2.043319</td>\n",
       "      <td>1.623150</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>3.418076</td>\n",
       "      <td>3.332634</td>\n",
       "      <td>...</td>\n",
       "      <td>4.559922</td>\n",
       "      <td>3.023272</td>\n",
       "      <td>1.478423</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>3.135162</td>\n",
       "      <td>1.429372</td>\n",
       "      <td>5.454369</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>3.010945</td>\n",
       "      <td>10.438015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>-15.043400</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>5.074800</td>\n",
       "      <td>-32.562600</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>5.349700</td>\n",
       "      <td>-10.505500</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.093300</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>-3.814500</td>\n",
       "      <td>-11.783400</td>\n",
       "      <td>8.694400</td>\n",
       "      <td>-5.261000</td>\n",
       "      <td>-14.209600</td>\n",
       "      <td>5.960600</td>\n",
       "      <td>6.299300</td>\n",
       "      <td>-38.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.453850</td>\n",
       "      <td>-4.740025</td>\n",
       "      <td>8.722475</td>\n",
       "      <td>5.254075</td>\n",
       "      <td>9.883175</td>\n",
       "      <td>-11.200350</td>\n",
       "      <td>4.767700</td>\n",
       "      <td>13.943800</td>\n",
       "      <td>-2.317800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>5.157400</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>15.629800</td>\n",
       "      <td>-1.170700</td>\n",
       "      <td>-1.946925</td>\n",
       "      <td>8.252800</td>\n",
       "      <td>13.829700</td>\n",
       "      <td>-11.208475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.524750</td>\n",
       "      <td>-1.608050</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>-4.833150</td>\n",
       "      <td>5.385100</td>\n",
       "      <td>16.456800</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>7.347750</td>\n",
       "      <td>1.901300</td>\n",
       "      <td>3.396350</td>\n",
       "      <td>17.957950</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>8.888200</td>\n",
       "      <td>15.934050</td>\n",
       "      <td>-2.819550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.758200</td>\n",
       "      <td>1.358625</td>\n",
       "      <td>12.516700</td>\n",
       "      <td>8.324100</td>\n",
       "      <td>12.261125</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.102900</td>\n",
       "      <td>2.937900</td>\n",
       "      <td>...</td>\n",
       "      <td>6.406200</td>\n",
       "      <td>9.512525</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>6.205800</td>\n",
       "      <td>20.396525</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>6.556725</td>\n",
       "      <td>9.593300</td>\n",
       "      <td>18.064725</td>\n",
       "      <td>4.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.315000</td>\n",
       "      <td>10.376800</td>\n",
       "      <td>19.353000</td>\n",
       "      <td>13.188300</td>\n",
       "      <td>16.671400</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>8.447700</td>\n",
       "      <td>27.691800</td>\n",
       "      <td>10.151300</td>\n",
       "      <td>...</td>\n",
       "      <td>18.440900</td>\n",
       "      <td>16.716500</td>\n",
       "      <td>8.402400</td>\n",
       "      <td>18.281800</td>\n",
       "      <td>27.928800</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>26.079100</td>\n",
       "      <td>28.500700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          var_0          var_1          var_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.100490      10.679914      -1.627622      10.715192   \n",
       "std         0.300653       3.040051       4.050044       2.640894   \n",
       "min         0.000000       0.408400     -15.043400       2.117100   \n",
       "25%         0.000000       8.453850      -4.740025       8.722475   \n",
       "50%         0.000000      10.524750      -1.608050      10.580000   \n",
       "75%         0.000000      12.758200       1.358625      12.516700   \n",
       "max         1.000000      20.315000      10.376800      19.353000   \n",
       "\n",
       "               var_3          var_4          var_5          var_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.796529      11.078333      -5.065317       5.408949   \n",
       "std         2.043319       1.623150       7.863267       0.866607   \n",
       "min        -0.040200       5.074800     -32.562600       2.347300   \n",
       "25%         5.254075       9.883175     -11.200350       4.767700   \n",
       "50%         6.825000      11.108250      -4.833150       5.385100   \n",
       "75%         8.324100      12.261125       0.924800       6.003000   \n",
       "max        13.188300      16.671400      17.251600       8.447700   \n",
       "\n",
       "               var_7          var_8      ...              var_190  \\\n",
       "count  200000.000000  200000.000000      ...        200000.000000   \n",
       "mean       16.545850       0.284162      ...             3.234440   \n",
       "std         3.418076       3.332634      ...             4.559922   \n",
       "min         5.349700     -10.505500      ...           -14.093300   \n",
       "25%        13.943800      -2.317800      ...            -0.058825   \n",
       "50%        16.456800       0.393700      ...             3.203600   \n",
       "75%        19.102900       2.937900      ...             6.406200   \n",
       "max        27.691800      10.151300      ...            18.440900   \n",
       "\n",
       "             var_191        var_192        var_193        var_194  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.438408       1.927839       3.331774      17.993784   \n",
       "std         3.023272       1.478423       3.992030       3.135162   \n",
       "min        -2.691700      -3.814500     -11.783400       8.694400   \n",
       "25%         5.157400       0.889775       0.584600      15.629800   \n",
       "50%         7.347750       1.901300       3.396350      17.957950   \n",
       "75%         9.512525       2.949500       6.205800      20.396525   \n",
       "max        16.716500       8.402400      18.281800      27.928800   \n",
       "\n",
       "             var_195        var_196        var_197        var_198  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.142088       2.303335       8.908158      15.870720   \n",
       "std         1.429372       5.454369       0.921625       3.010945   \n",
       "min        -5.261000     -14.209600       5.960600       6.299300   \n",
       "25%        -1.170700      -1.946925       8.252800      13.829700   \n",
       "50%        -0.172700       2.408900       8.888200      15.934050   \n",
       "75%         0.829600       6.556725       9.593300      18.064725   \n",
       "max         4.272900      18.321500      12.000400      26.079100   \n",
       "\n",
       "             var_199  \n",
       "count  200000.000000  \n",
       "mean       -3.326537  \n",
       "std        10.438015  \n",
       "min       -38.852800  \n",
       "25%       -11.208475  \n",
       "50%        -2.819550  \n",
       "75%         4.836800  \n",
       "max        28.500700  \n",
       "\n",
       "[8 rows x 201 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n",
    "target = train_df['target']\n",
    "param = {\n",
    "    'bagging_freq': 5,          'bagging_fraction': 0.38,   'boost_from_average':'false',   'boost': 'gbdt',\n",
    "    'feature_fraction': 0.045,   'learning_rate': 0.0105,     'max_depth': -1,                'metric':'auc',\n",
    "    'min_data_in_leaf': 80,     'min_sum_hessian_in_leaf': 10.0,'num_leaves': 13,           'num_threads': 8,\n",
    "    'tree_learner': 'serial',   'objective': 'binary',      'verbosity': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits = 12, shuffle = False, random_state = 44000)\n",
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold :1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901299\tvalid_1's auc: 0.882845\n",
      "[2000]\ttraining's auc: 0.912408\tvalid_1's auc: 0.890959\n",
      "[3000]\ttraining's auc: 0.91966\tvalid_1's auc: 0.895358\n",
      "[4000]\ttraining's auc: 0.925196\tvalid_1's auc: 0.897666\n",
      "[5000]\ttraining's auc: 0.929874\tvalid_1's auc: 0.89883\n",
      "[6000]\ttraining's auc: 0.934074\tvalid_1's auc: 0.899669\n",
      "[7000]\ttraining's auc: 0.937927\tvalid_1's auc: 0.900239\n",
      "[8000]\ttraining's auc: 0.941703\tvalid_1's auc: 0.900637\n",
      "[9000]\ttraining's auc: 0.945214\tvalid_1's auc: 0.900805\n",
      "[10000]\ttraining's auc: 0.948602\tvalid_1's auc: 0.9007\n",
      "Early stopping, best iteration is:\n",
      "[9281]\ttraining's auc: 0.946207\tvalid_1's auc: 0.900846\n",
      "Fold :2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.900982\tvalid_1's auc: 0.885907\n",
      "[2000]\ttraining's auc: 0.912388\tvalid_1's auc: 0.891921\n",
      "[3000]\ttraining's auc: 0.919733\tvalid_1's auc: 0.894824\n",
      "[4000]\ttraining's auc: 0.925317\tvalid_1's auc: 0.89661\n",
      "[5000]\ttraining's auc: 0.930093\tvalid_1's auc: 0.897642\n",
      "[6000]\ttraining's auc: 0.934365\tvalid_1's auc: 0.898061\n",
      "[7000]\ttraining's auc: 0.938298\tvalid_1's auc: 0.89828\n",
      "[8000]\ttraining's auc: 0.941948\tvalid_1's auc: 0.898368\n",
      "[9000]\ttraining's auc: 0.945431\tvalid_1's auc: 0.898458\n",
      "[10000]\ttraining's auc: 0.948821\tvalid_1's auc: 0.898495\n",
      "Early stopping, best iteration is:\n",
      "[9737]\ttraining's auc: 0.947931\tvalid_1's auc: 0.898638\n",
      "Fold :3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902074\tvalid_1's auc: 0.87564\n",
      "[2000]\ttraining's auc: 0.913333\tvalid_1's auc: 0.883429\n",
      "[3000]\ttraining's auc: 0.920453\tvalid_1's auc: 0.887207\n",
      "[4000]\ttraining's auc: 0.925949\tvalid_1's auc: 0.889457\n",
      "[5000]\ttraining's auc: 0.930705\tvalid_1's auc: 0.890597\n",
      "[6000]\ttraining's auc: 0.934858\tvalid_1's auc: 0.891287\n",
      "[7000]\ttraining's auc: 0.938715\tvalid_1's auc: 0.89177\n",
      "[8000]\ttraining's auc: 0.942306\tvalid_1's auc: 0.891985\n",
      "[9000]\ttraining's auc: 0.945815\tvalid_1's auc: 0.892271\n",
      "[10000]\ttraining's auc: 0.949146\tvalid_1's auc: 0.892347\n",
      "Early stopping, best iteration is:\n",
      "[9948]\ttraining's auc: 0.94898\tvalid_1's auc: 0.892404\n",
      "Fold :4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.900371\tvalid_1's auc: 0.888888\n",
      "[2000]\ttraining's auc: 0.911711\tvalid_1's auc: 0.897763\n",
      "[3000]\ttraining's auc: 0.919\tvalid_1's auc: 0.901684\n",
      "[4000]\ttraining's auc: 0.924709\tvalid_1's auc: 0.903837\n",
      "[5000]\ttraining's auc: 0.929508\tvalid_1's auc: 0.905146\n",
      "[6000]\ttraining's auc: 0.933701\tvalid_1's auc: 0.905907\n",
      "[7000]\ttraining's auc: 0.937621\tvalid_1's auc: 0.906155\n",
      "[8000]\ttraining's auc: 0.941338\tvalid_1's auc: 0.906258\n",
      "[9000]\ttraining's auc: 0.944821\tvalid_1's auc: 0.906223\n",
      "Early stopping, best iteration is:\n",
      "[8625]\ttraining's auc: 0.943528\tvalid_1's auc: 0.906414\n",
      "Fold :5\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901262\tvalid_1's auc: 0.876285\n",
      "[2000]\ttraining's auc: 0.912634\tvalid_1's auc: 0.884969\n",
      "[3000]\ttraining's auc: 0.920026\tvalid_1's auc: 0.888695\n",
      "[4000]\ttraining's auc: 0.92568\tvalid_1's auc: 0.89071\n",
      "[5000]\ttraining's auc: 0.930396\tvalid_1's auc: 0.891902\n",
      "[6000]\ttraining's auc: 0.934546\tvalid_1's auc: 0.892598\n",
      "[7000]\ttraining's auc: 0.938453\tvalid_1's auc: 0.892793\n",
      "[8000]\ttraining's auc: 0.942073\tvalid_1's auc: 0.892944\n",
      "[9000]\ttraining's auc: 0.94556\tvalid_1's auc: 0.893082\n",
      "[10000]\ttraining's auc: 0.948864\tvalid_1's auc: 0.89321\n",
      "[11000]\ttraining's auc: 0.952037\tvalid_1's auc: 0.89315\n",
      "Early stopping, best iteration is:\n",
      "[10742]\ttraining's auc: 0.951207\tvalid_1's auc: 0.89332\n",
      "Fold :6\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.900763\tvalid_1's auc: 0.884081\n",
      "[2000]\ttraining's auc: 0.91223\tvalid_1's auc: 0.892423\n",
      "[3000]\ttraining's auc: 0.91949\tvalid_1's auc: 0.896244\n",
      "[4000]\ttraining's auc: 0.925111\tvalid_1's auc: 0.898246\n",
      "[5000]\ttraining's auc: 0.929849\tvalid_1's auc: 0.898951\n",
      "[6000]\ttraining's auc: 0.934112\tvalid_1's auc: 0.89934\n",
      "[7000]\ttraining's auc: 0.938013\tvalid_1's auc: 0.899648\n",
      "[8000]\ttraining's auc: 0.941713\tvalid_1's auc: 0.899882\n",
      "[9000]\ttraining's auc: 0.945186\tvalid_1's auc: 0.899863\n",
      "[10000]\ttraining's auc: 0.948563\tvalid_1's auc: 0.899746\n",
      "Early stopping, best iteration is:\n",
      "[9150]\ttraining's auc: 0.945709\tvalid_1's auc: 0.899972\n",
      "Fold :7\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.900451\tvalid_1's auc: 0.88687\n",
      "[2000]\ttraining's auc: 0.911881\tvalid_1's auc: 0.894773\n",
      "[3000]\ttraining's auc: 0.91921\tvalid_1's auc: 0.8984\n",
      "[4000]\ttraining's auc: 0.924888\tvalid_1's auc: 0.900155\n",
      "[5000]\ttraining's auc: 0.929657\tvalid_1's auc: 0.901289\n",
      "[6000]\ttraining's auc: 0.933886\tvalid_1's auc: 0.901831\n",
      "[7000]\ttraining's auc: 0.937815\tvalid_1's auc: 0.902026\n",
      "[8000]\ttraining's auc: 0.941513\tvalid_1's auc: 0.901937\n",
      "Early stopping, best iteration is:\n",
      "[7068]\ttraining's auc: 0.938056\tvalid_1's auc: 0.902082\n",
      "Fold :8\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.900456\tvalid_1's auc: 0.890627\n",
      "[2000]\ttraining's auc: 0.911675\tvalid_1's auc: 0.897718\n",
      "[3000]\ttraining's auc: 0.919104\tvalid_1's auc: 0.900785\n",
      "[4000]\ttraining's auc: 0.924818\tvalid_1's auc: 0.902585\n",
      "[5000]\ttraining's auc: 0.929621\tvalid_1's auc: 0.903323\n",
      "[6000]\ttraining's auc: 0.93387\tvalid_1's auc: 0.903683\n",
      "[7000]\ttraining's auc: 0.937796\tvalid_1's auc: 0.903652\n",
      "Early stopping, best iteration is:\n",
      "[6167]\ttraining's auc: 0.934539\tvalid_1's auc: 0.903839\n",
      "Fold :9\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.9011\tvalid_1's auc: 0.880143\n",
      "[2000]\ttraining's auc: 0.912301\tvalid_1's auc: 0.888948\n",
      "[3000]\ttraining's auc: 0.919614\tvalid_1's auc: 0.89322\n",
      "[4000]\ttraining's auc: 0.925266\tvalid_1's auc: 0.895547\n",
      "[5000]\ttraining's auc: 0.930027\tvalid_1's auc: 0.89709\n",
      "[6000]\ttraining's auc: 0.934203\tvalid_1's auc: 0.897728\n",
      "[7000]\ttraining's auc: 0.938071\tvalid_1's auc: 0.898082\n",
      "[8000]\ttraining's auc: 0.941756\tvalid_1's auc: 0.89822\n",
      "[9000]\ttraining's auc: 0.945189\tvalid_1's auc: 0.898231\n",
      "[10000]\ttraining's auc: 0.948538\tvalid_1's auc: 0.898219\n",
      "Early stopping, best iteration is:\n",
      "[9228]\ttraining's auc: 0.945958\tvalid_1's auc: 0.898424\n",
      "Fold :10\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.900593\tvalid_1's auc: 0.88788\n",
      "[2000]\ttraining's auc: 0.911765\tvalid_1's auc: 0.896002\n",
      "[3000]\ttraining's auc: 0.919135\tvalid_1's auc: 0.899881\n",
      "[4000]\ttraining's auc: 0.924739\tvalid_1's auc: 0.901765\n",
      "[5000]\ttraining's auc: 0.929534\tvalid_1's auc: 0.903005\n",
      "[6000]\ttraining's auc: 0.933801\tvalid_1's auc: 0.903306\n",
      "[7000]\ttraining's auc: 0.937675\tvalid_1's auc: 0.90369\n",
      "[8000]\ttraining's auc: 0.941402\tvalid_1's auc: 0.903667\n",
      "[9000]\ttraining's auc: 0.944839\tvalid_1's auc: 0.903687\n",
      "Early stopping, best iteration is:\n",
      "[8175]\ttraining's auc: 0.942017\tvalid_1's auc: 0.903818\n",
      "Fold :11\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.900674\tvalid_1's auc: 0.890072\n",
      "[2000]\ttraining's auc: 0.9115\tvalid_1's auc: 0.897793\n",
      "[3000]\ttraining's auc: 0.918974\tvalid_1's auc: 0.902107\n",
      "[4000]\ttraining's auc: 0.92456\tvalid_1's auc: 0.904552\n",
      "[5000]\ttraining's auc: 0.929272\tvalid_1's auc: 0.905824\n",
      "[6000]\ttraining's auc: 0.933472\tvalid_1's auc: 0.906602\n",
      "[7000]\ttraining's auc: 0.937472\tvalid_1's auc: 0.907344\n",
      "[8000]\ttraining's auc: 0.941126\tvalid_1's auc: 0.907396\n",
      "Early stopping, best iteration is:\n",
      "[7272]\ttraining's auc: 0.938448\tvalid_1's auc: 0.907576\n",
      "Fold :12\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901185\tvalid_1's auc: 0.885261\n",
      "[2000]\ttraining's auc: 0.912398\tvalid_1's auc: 0.89181\n",
      "[3000]\ttraining's auc: 0.919765\tvalid_1's auc: 0.895383\n",
      "[4000]\ttraining's auc: 0.925429\tvalid_1's auc: 0.89762\n",
      "[5000]\ttraining's auc: 0.93012\tvalid_1's auc: 0.898548\n",
      "[6000]\ttraining's auc: 0.93427\tvalid_1's auc: 0.898871\n",
      "[7000]\ttraining's auc: 0.938153\tvalid_1's auc: 0.899407\n",
      "Early stopping, best iteration is:\n",
      "[6535]\ttraining's auc: 0.936387\tvalid_1's auc: 0.899512\n",
      "CV score: 0.90025 CV score: 0.90025 \n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n",
    "    print(\"Fold :{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label = target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label = target.iloc[val_idx])\n",
    "    clf = lgb.train(param, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval = 1000, early_stopping_rounds = 1000)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration = clf.best_iteration)\n",
    "    predictions += clf.predict(test_df[features], num_iteration = clf.best_iteration) / folds.n_splits\n",
    "sys.stdout.write(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "#sub[\"target\"] = predictions\n",
    "#sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df[train_df.columns.values[1:]], \n",
    "                                                    train_df[train_df.columns.values[0]], \n",
    "                                                    test_size = 0.3, random_state = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 200) (60000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dind-vm/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# work with X_train, X_test, y_train, y_test\n",
    "# Create an object of Logistic Regression with parameters C and class_weight\n",
    "logist = LogisticRegression(C = 0.001, class_weight = 'balanced')\n",
    "\n",
    "# Fit the training data on this object\n",
    "logist.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Target for validation dataset \n",
    "logist_pred = logist.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(Y_test, logist_pred):\n",
    "    logist_pred_var = [0 if i < 0.5 else 1 for i in logist_pred]\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(Y_test, logist_pred_var)) \n",
    "      \n",
    "    #print(classification_report(Y_test, logist_pred)) \n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, logist_pred, pos_label = 1)\n",
    "    print('AUC:')\n",
    "    print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[42214 11737]\n",
      " [ 1404  4645]]\n",
      "AUC:\n",
      "0.8541970967765768\n"
     ]
    }
   ],
   "source": [
    "performance(y_test, logist_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
       "            max_depth=None, max_features=0.7, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=80, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=2019,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Decision Tree Classifier object with few parameters\n",
    "tree_clf = DecisionTreeClassifier(class_weight = 'balanced', random_state = 2019, \n",
    "                                  max_features = 0.7, min_samples_leaf = 80)\n",
    "\n",
    "# Fit the object on training data\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[35314 18637]\n",
      " [ 2629  3420]]\n",
      "AUC:\n",
      "0.6514727493199708\n"
     ]
    }
   ],
   "source": [
    "# Predict for validation set and check the performance\n",
    "tree_preds = tree_clf.predict_proba(X_test)[:, 1]\n",
    "performance(y_test, tree_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 28.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features=0.5,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=100,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=None, oob_score=False,\n",
       "            random_state=2019, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random Forest Object using the mentioned parameters\n",
    "random_forest = RandomForestClassifier(n_estimators = 100, random_state = 2019, verbose = 1,\n",
    "                                      class_weight = 'balanced', max_features = 0.5, \n",
    "                                       min_samples_leaf = 100)\n",
    "\n",
    "# Fit the object on training set \n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[46548  7403]\n",
      " [ 2978  3071]]\n",
      "AUC:\n",
      "0.7872813411975419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Predict the validation set target and check the performance\n",
    "forest_preds = random_forest.predict_proba(X_test)[:, 1]\n",
    "performance(y_test, forest_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, RepeatVector, Lambda, Permute, Activation, Masking, Reshape\n",
    "from keras.layers import recurrent, Input, TimeDistributed, add, concatenate, Multiply, Bidirectional\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.activations import softmax\n",
    "from keras.metrics import categorical_accuracy\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras import initializers\n",
    "#from keras.utils.visualize_util import plot\n",
    "from keras.layers.core import Layer\n",
    "\n",
    "from keras.activations import softmax, tanh, sigmoid, hard_sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(in_shape):\n",
    "    adam = Adam(lr = 0.0003)  # Best learning found in previous exp\n",
    "    input_layer = Input(shape = (in_shape,), name = 'input_layer')\n",
    "    dense = Dense(100, activation = 'relu')(input_layer)\n",
    "    #dense = Dropout(0.35)(dense)\n",
    "    dense = Dense(50, activation = 'relu')(dense)\n",
    "    dense = Dropout(0.30)(dense)\n",
    "    output = Dense(2, activation = 'softmax')(dense)\n",
    "    model = Model(inputs = input_layer, outputs = output)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    #model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "    #tf_thetas = tf.get_variable(\"tf_thetas\",\n",
    "    #                        initializer=thetas)\n",
    "\n",
    "    #sample_output = Lambda(lambda x: \n",
    "    #               gumbel_softmax(x, temperature,  hard = hard_val), \n",
    "    #               output_shape = (2,))(output)\n",
    "    #cond_prob = Lambda(lambda x: tf.einsum('ai,ij->aj', x[0], tf_thetas*1.),\n",
    "    #          output_shape = (2, ))([sample_output])\n",
    "\n",
    "    #model_ask = Model(inputs = input_layer, outputs = cond_prob)\n",
    "    #model_ask.compile(loss = 'categorical_crossentropy', \n",
    "    #              optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model#, model_ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.choice(len(X_train), len(X_train), replace = False)\n",
    "\n",
    "X_train_nn = X_train.iloc[random_idx]\n",
    "y_train_nn = y_train.iloc[random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = create_models(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_binary_train = to_categorical(y_train)\n",
    "y_binary_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140000 samples, validate on 60000 samples\n",
      "Epoch 1/50\n",
      "140000/140000 [==============================] - 18s 125us/step - loss: 0.3008 - acc: 0.8999 - val_loss: 0.2594 - val_acc: 0.9028\n",
      "Epoch 2/50\n",
      "140000/140000 [==============================] - 14s 103us/step - loss: 0.2606 - acc: 0.9064 - val_loss: 0.2495 - val_acc: 0.9092\n",
      "Epoch 3/50\n",
      "140000/140000 [==============================] - 14s 100us/step - loss: 0.2562 - acc: 0.9072 - val_loss: 0.2474 - val_acc: 0.9088\n",
      "Epoch 4/50\n",
      "140000/140000 [==============================] - 14s 102us/step - loss: 0.2541 - acc: 0.9072 - val_loss: 0.2458 - val_acc: 0.9095\n",
      "Epoch 5/50\n",
      "140000/140000 [==============================] - 14s 99us/step - loss: 0.2514 - acc: 0.9075 - val_loss: 0.2460 - val_acc: 0.9097\n",
      "Epoch 6/50\n",
      "140000/140000 [==============================] - 14s 103us/step - loss: 0.2505 - acc: 0.9086 - val_loss: 0.2489 - val_acc: 0.9095\n",
      "Epoch 7/50\n",
      "140000/140000 [==============================] - 13s 91us/step - loss: 0.2499 - acc: 0.9091 - val_loss: 0.2462 - val_acc: 0.9089\n",
      "Epoch 8/50\n",
      "140000/140000 [==============================] - 14s 99us/step - loss: 0.2491 - acc: 0.9091 - val_loss: 0.2489 - val_acc: 0.9074\n",
      "Epoch 9/50\n",
      "140000/140000 [==============================] - 13s 95us/step - loss: 0.2489 - acc: 0.9086 - val_loss: 0.2463 - val_acc: 0.9091\n",
      "Epoch 10/50\n",
      "140000/140000 [==============================] - 15s 111us/step - loss: 0.2485 - acc: 0.9085 - val_loss: 0.2424 - val_acc: 0.9103\n",
      "Epoch 11/50\n",
      "140000/140000 [==============================] - 14s 103us/step - loss: 0.2470 - acc: 0.9100 - val_loss: 0.2429 - val_acc: 0.9104\n",
      "Epoch 12/50\n",
      "140000/140000 [==============================] - 15s 106us/step - loss: 0.2468 - acc: 0.9098 - val_loss: 0.2471 - val_acc: 0.9087\n",
      "Epoch 13/50\n",
      "140000/140000 [==============================] - 14s 102us/step - loss: 0.2460 - acc: 0.9097 - val_loss: 0.2442 - val_acc: 0.9101\n",
      "Epoch 14/50\n",
      "140000/140000 [==============================] - 14s 101us/step - loss: 0.2458 - acc: 0.9094 - val_loss: 0.2422 - val_acc: 0.9103\n",
      "Epoch 15/50\n",
      "140000/140000 [==============================] - 14s 102us/step - loss: 0.2448 - acc: 0.9102 - val_loss: 0.2454 - val_acc: 0.9108\n",
      "Epoch 16/50\n",
      "140000/140000 [==============================] - 15s 109us/step - loss: 0.2454 - acc: 0.9103 - val_loss: 0.2458 - val_acc: 0.9088\n",
      "Epoch 17/50\n",
      "140000/140000 [==============================] - 15s 107us/step - loss: 0.2445 - acc: 0.9110 - val_loss: 0.2567 - val_acc: 0.9082\n",
      "Epoch 18/50\n",
      "140000/140000 [==============================] - 14s 98us/step - loss: 0.2447 - acc: 0.9102 - val_loss: 0.2444 - val_acc: 0.9101\n",
      "Epoch 19/50\n",
      "140000/140000 [==============================] - 14s 97us/step - loss: 0.2437 - acc: 0.9111 - val_loss: 0.2439 - val_acc: 0.9112\n",
      "Epoch 20/50\n",
      "140000/140000 [==============================] - 14s 101us/step - loss: 0.2437 - acc: 0.9105 - val_loss: 0.2470 - val_acc: 0.9098\n",
      "Epoch 21/50\n",
      "140000/140000 [==============================] - 14s 103us/step - loss: 0.2433 - acc: 0.9112 - val_loss: 0.2460 - val_acc: 0.9066\n",
      "Epoch 22/50\n",
      "140000/140000 [==============================] - 14s 100us/step - loss: 0.2432 - acc: 0.9107 - val_loss: 0.2435 - val_acc: 0.9103\n",
      "Epoch 23/50\n",
      "140000/140000 [==============================] - 14s 98us/step - loss: 0.2440 - acc: 0.9104 - val_loss: 0.2444 - val_acc: 0.9099\n",
      "Epoch 24/50\n",
      "140000/140000 [==============================] - 16s 111us/step - loss: 0.2430 - acc: 0.9109 - val_loss: 0.2467 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      "140000/140000 [==============================] - 14s 98us/step - loss: 0.2428 - acc: 0.9112 - val_loss: 0.2416 - val_acc: 0.9108\n",
      "Epoch 26/50\n",
      "140000/140000 [==============================] - 15s 105us/step - loss: 0.2428 - acc: 0.9110 - val_loss: 0.2644 - val_acc: 0.9073\n",
      "Epoch 27/50\n",
      "140000/140000 [==============================] - 14s 102us/step - loss: 0.2424 - acc: 0.9112 - val_loss: 0.2417 - val_acc: 0.9110\n",
      "Epoch 28/50\n",
      "140000/140000 [==============================] - 15s 108us/step - loss: 0.2425 - acc: 0.9107 - val_loss: 0.2556 - val_acc: 0.9055\n",
      "Epoch 29/50\n",
      "140000/140000 [==============================] - 14s 99us/step - loss: 0.2428 - acc: 0.9109 - val_loss: 0.2428 - val_acc: 0.9096\n",
      "Epoch 30/50\n",
      "140000/140000 [==============================] - 15s 105us/step - loss: 0.2420 - acc: 0.9108 - val_loss: 0.2413 - val_acc: 0.9111\n",
      "Epoch 31/50\n",
      "140000/140000 [==============================] - 14s 100us/step - loss: 0.2420 - acc: 0.9111 - val_loss: 0.2433 - val_acc: 0.9100\n",
      "Epoch 32/50\n",
      "140000/140000 [==============================] - 15s 106us/step - loss: 0.2424 - acc: 0.9113 - val_loss: 0.2423 - val_acc: 0.9099\n",
      "Epoch 33/50\n",
      "140000/140000 [==============================] - 14s 101us/step - loss: 0.2419 - acc: 0.9115 - val_loss: 0.2455 - val_acc: 0.9092\n",
      "Epoch 34/50\n",
      "140000/140000 [==============================] - 16s 111us/step - loss: 0.2415 - acc: 0.9117 - val_loss: 0.2438 - val_acc: 0.9096\n",
      "Epoch 35/50\n",
      "140000/140000 [==============================] - 16s 116us/step - loss: 0.2420 - acc: 0.9115 - val_loss: 0.2430 - val_acc: 0.9104\n",
      "Epoch 36/50\n",
      "140000/140000 [==============================] - 15s 107us/step - loss: 0.2414 - acc: 0.9113 - val_loss: 0.2418 - val_acc: 0.9098\n",
      "Epoch 37/50\n",
      "140000/140000 [==============================] - 15s 108us/step - loss: 0.2415 - acc: 0.9118 - val_loss: 0.2415 - val_acc: 0.9109\n",
      "Epoch 38/50\n",
      "140000/140000 [==============================] - 15s 108us/step - loss: 0.2414 - acc: 0.9112 - val_loss: 0.2412 - val_acc: 0.9115\n",
      "Epoch 39/50\n",
      "140000/140000 [==============================] - 14s 99us/step - loss: 0.2416 - acc: 0.9110 - val_loss: 0.2428 - val_acc: 0.9103\n",
      "Epoch 40/50\n",
      "140000/140000 [==============================] - 14s 103us/step - loss: 0.2415 - acc: 0.9112 - val_loss: 0.2449 - val_acc: 0.9098\n",
      "Epoch 41/50\n",
      "140000/140000 [==============================] - 15s 108us/step - loss: 0.2414 - acc: 0.9111 - val_loss: 0.2471 - val_acc: 0.9095\n",
      "Epoch 42/50\n",
      "140000/140000 [==============================] - 15s 104us/step - loss: 0.2407 - acc: 0.9116 - val_loss: 0.2493 - val_acc: 0.9087\n",
      "Epoch 43/50\n",
      "140000/140000 [==============================] - 15s 111us/step - loss: 0.2406 - acc: 0.9118 - val_loss: 0.2488 - val_acc: 0.9105\n",
      "Epoch 44/50\n",
      "140000/140000 [==============================] - 15s 107us/step - loss: 0.2405 - acc: 0.9119 - val_loss: 0.2402 - val_acc: 0.9114\n",
      "Epoch 45/50\n",
      "140000/140000 [==============================] - 15s 107us/step - loss: 0.2407 - acc: 0.9117 - val_loss: 0.2406 - val_acc: 0.9106\n",
      "Epoch 46/50\n",
      "140000/140000 [==============================] - 15s 105us/step - loss: 0.2405 - acc: 0.9114 - val_loss: 0.2477 - val_acc: 0.9072\n",
      "Epoch 47/50\n",
      "140000/140000 [==============================] - 14s 102us/step - loss: 0.2405 - acc: 0.9119 - val_loss: 0.2487 - val_acc: 0.9085\n",
      "Epoch 48/50\n",
      "140000/140000 [==============================] - 15s 108us/step - loss: 0.2405 - acc: 0.9117 - val_loss: 0.2536 - val_acc: 0.9081\n",
      "Epoch 49/50\n",
      "140000/140000 [==============================] - 14s 103us/step - loss: 0.2404 - acc: 0.9121 - val_loss: 0.2470 - val_acc: 0.9100\n",
      "Epoch 50/50\n",
      "140000/140000 [==============================] - 15s 104us/step - loss: 0.2402 - acc: 0.9117 - val_loss: 0.2427 - val_acc: 0.9098\n"
     ]
    }
   ],
   "source": [
    "# Make severl models, computes and average them. Consider Kfold, SVM, NN, Logisteic Regression, Light GBM and several configurations of them\n",
    "h = model.fit(X_train, y_binary_train, \n",
    "              epochs = 50, validation_data = (X_test, y_binary_test),\n",
    "              batch_size = 25,\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_nn = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nn_predic = []\n",
    "for i in range(len(y_predicted_nn)):\n",
    "    if y_predicted_nn[i,0] > y_predicted_nn[i,1]:\n",
    "        temp_nn_predic.append(0)\n",
    "    else:\n",
    "        temp_nn_predic.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# There is no gain with more layers!\n",
    "fpr_nn, tpr_nn, threshold_nn = roc_curve(y_test, temp_nn_predic)\n",
    "roc_auc_nn = auc(fpr_nn, tpr_nn)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_nn, tpr_nn, 'b', label = 'AUC = %0.2f' % roc_auc_nn)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
